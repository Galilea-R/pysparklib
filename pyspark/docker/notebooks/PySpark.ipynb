{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "## SparkSession\n",
    "> Las aplicaciones de PySpark comienzan inicializando `SparkSession`, que es el punto de entrada de PySpark, como se muestra a continuación. En el caso de ejecutarlo en la consola de PySpark mediante el ejecutable `pyspark`, la consola crea automáticamente la sesión en la variable `spark` para los usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa la clase SparkSession del módulo pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "# Crea o recupera una instancia de SparkSession con el nombre de la aplicación \"example\"\n",
    "# SparkSession es el punto de entrada principal para trabajar con datos en PySpark\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del DataFrame\n",
    "> Para empezar, puedes crear un DataFrame en PySpark a partir de una lista de filas. Si no tiene un esquema específico, lo infiere automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa las clases datetime y date del módulo datetime y la clase Row del módulo pyspark.sql\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "# Crea un DataFrame en PySpark a partir de una lista de filas utilizando la instancia de SparkSession llamada 'spark'\n",
    "df = spark.createDataFrame([\n",
    "    # Cada elemento de la lista es una instancia de la clase Row que representa una fila en el DataFrame\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=3, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "# El DataFrame 'df' se crea con éxito y ahora puedes realizar operaciones en él\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Crea un DataFrame en PySpark con un esquema explícito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crea un DataFrame en PySpark con un esquema explícito utilizando la instancia de SparkSession llamada 'spark'\n",
    "df1 = spark.createDataFrame([\n",
    "    # Cada elemento de la lista es una tupla que representa una fila en el DataFrame\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "# El DataFrame 'df1' se crea con éxito y ahora puedes realizar operaciones en él\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Create a PySpark DataFrame from a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crea un DataFrame en PySpark con un esquema explícito utilizando la instancia de SparkSession llamada 'spark'\n",
    "df2 = spark.createDataFrame([\n",
    "    # Cada elemento de la lista es una tupla que representa una fila en el DataFrame\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "# El DataFrame 'df1' se crea con éxito y ahora puedes realizar operaciones en él\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura\n",
    ">CSV. Requiere que digas explícitamente si trae nombres de columnas con el header y si quieres que infiera el tipo de datos con inferSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|experience|salary|\n",
      "+---------+---+----------+------+\n",
      "| Mauricio| 21|         0| 30000|\n",
      "| Santiago| 35|         3|  4000|\n",
      "|   Carlos| 54|        21| 15000|\n",
      "|   Javier| 26|         6| 18000|\n",
      "|  Valeria| 45|        13| 20100|\n",
      "|Francisco| 34|        21| 30000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcsv = spark.read.csv('datos.csv', header=True, inferSchema=True)\n",
    "dfcsv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Parquet. No requiere información extra, porque ya está almacenada directo en la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|experience|salary|\n",
      "+---------+---+----------+------+\n",
      "| Mauricio| 21|         0| 30000|\n",
      "| Santiago| 35|         3|  4000|\n",
      "|   Carlos| 54|        21| 15000|\n",
      "|   Javier| 26|         6| 18000|\n",
      "|  Valeria| 45|        13| 20100|\n",
      "|Francisco| 34|        21| 30000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfpar = spark.read.parquet('datos.parquet')\n",
    "dfpar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formas de ver el DF\n",
    "> Un DataFrame se puede mostrar utilizando DataFrame.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Las primeras filas de un DataFrame pueden mostrarse con DataFrame.show(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Las filas también se pueden mostrar verticalmente. Esto es útil cuando las filas son demasiado largas para mostrarse horizontalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | string2             \n",
      " d   | 2000-02-01          \n",
      " e   | 2000-01-02 12:00:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Puedes ver el esquema del DataFrame y los nombres de las columnas de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Muestra el resumen del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------------+-------+\n",
      "|summary|  a|                 b|      c|\n",
      "+-------+---+------------------+-------+\n",
      "|  count|  3|                 3|      3|\n",
      "|   mean|2.0|3.3333333333333335|   NULL|\n",
      "| stddev|1.0|1.5275252316519465|   NULL|\n",
      "|    min|  1|               2.0|string1|\n",
      "|    max|  3|               5.0|string3|\n",
      "+-------+---+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"a\", \"b\", \"c\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">`DataFrame.collect()` recopila los datos distribuidos del driver como datos locales en Python. Ten en cuenta que esto puede generar un error de falta de memoria cuando el conjunto de datos es demasiado grande para ajustarse en el driver, ya que recopila todos los datos de los ejecutores en el driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=3, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Para evitar lanzar una excepción de falta de memoria, utiliza `DataFrame.take()` o `DataFrame.tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2) # Devuelve una lista de las primeras 2 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=3, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2) # Devuelve una lista de las últimas 2 filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">El DataFrame de PySpark también permite la conversión a un DataFrame de pandas para aprovechar la API de pandas. Ten en cuenta que `toPandas` también recopila todos los datos en el lado del controlador, lo que puede causar fácilmente un error de falta de memoria cuando los datos son demasiado grandes para ajustarse en el lado del controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\anaconda3\\envs\\sparkenv\\Lib\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "c:\\Users\\santi\\anaconda3\\envs\\sparkenv\\Lib\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  3  5.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección, acceso y filtro a los datos\n",
    ">El DataFrame de PySpark se evalúa de forma lazy y selecciona una columna no desencadena la computación, sino que devuelve una instancia de Columna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Estas Columnas se pueden utilizar para seleccionar las columnas de un DataFrame. Por ejemplo, `DataFrame.select()` toma las instancias de Columna que devuelven otro DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|string1|\n",
      "|string2|\n",
      "|string3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.c).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Para seleccionar un subconjunto de filas, utiliza `DataFrame.filter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Para esta parte utilizaremos el DF obtenido del csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|experience|salary|\n",
      "+---------+---+----------+------+\n",
      "| Mauricio| 21|         0| 30000|\n",
      "| Santiago| 35|         3|  4000|\n",
      "|   Carlos| 54|        21| 15000|\n",
      "|   Javier| 26|         6| 18000|\n",
      "|  Valeria| 45|        13| 20100|\n",
      "|Francisco| 34|        21| 30000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcsv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Se puede utilizar esta notación para hacer filtros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|experience|salary|\n",
      "+---------+---+----------+------+\n",
      "| Mauricio| 21|         0| 30000|\n",
      "|  Valeria| 45|        13| 20100|\n",
      "|Francisco| 34|        21| 30000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "+---------+---+\n",
      "|     Name|age|\n",
      "+---------+---+\n",
      "| Mauricio| 21|\n",
      "|  Valeria| 45|\n",
      "|Francisco| 34|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcsv.filter('salary>=20000').show() #filtra los registros que cumplan la condición\n",
    "dfcsv.filter('salary>=20000').select(['Name', 'age']).show() #filtra los registros que cumplan la condición, limitando el número de columnas seleccionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">También puede utilizarse notación de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|experience|salary|\n",
      "+---------+---+----------+------+\n",
      "| Mauricio| 21|         0| 30000|\n",
      "|  Valeria| 45|        13| 20100|\n",
      "|Francisco| 34|        21| 30000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "+------+---+----------+------+\n",
      "|  Name|age|experience|salary|\n",
      "+------+---+----------+------+\n",
      "|Carlos| 54|        21| 15000|\n",
      "|Javier| 26|         6| 18000|\n",
      "+------+---+----------+------+\n",
      "\n",
      "+--------+---+----------+------+\n",
      "|    Name|age|experience|salary|\n",
      "+--------+---+----------+------+\n",
      "|Santiago| 35|         3|  4000|\n",
      "|  Carlos| 54|        21| 15000|\n",
      "|  Javier| 26|         6| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcsv.filter(dfcsv['salary']>=20000).show() #otra forma de hacerlo con notación de pandas\n",
    "dfcsv.filter((dfcsv['salary']<=20000)&(dfcsv['salary']>=10000)).show() #puedes anidar condiciones con & y |\n",
    "dfcsv.filter(~(dfcsv['salary']>=20000)).show() #negación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando una función\n",
    "> PySpark admite diversas UDFs (funciones definidas por el usuario) y APIs que permiten a los usuarios ejecutar funciones nativas de Python. Consulta también las últimas UDFs de Pandas y las APIs de Funciones de Pandas. Por ejemplo, el siguiente ejemplo permite a los usuarios utilizar directamente las APIs en una Serie de pandas dentro de una función nativa de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(a)|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|                 3|\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "df2.select(pandas_plus_one(df2.a)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Otro ejemplo es `DataFrame.mapInPandas`, que permite a los usuarios utilizar directamente las APIs en un DataFrame de pandas sin restricciones, como la longitud del resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define una función de filtro de Pandas llamada pandas_filter_func que toma un iterador como entrada.\n",
    "def pandas_filter_func(iterator):\n",
    "    for pandas_df in iterator:\n",
    "        # Filtra las filas del DataFrame de Pandas donde la columna 'a' es igual a 1.\n",
    "        yield pandas_df[pandas_df.a == 1]\n",
    "# Aplica la función de filtro a cada partición del DataFrame 'df' utilizando mapInPandas,\n",
    "# y muestra el resultado con el mismo esquema que el DataFrame original.\n",
    "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data\n",
    "> El DataFrame de PySpark también ofrece una forma de manejar datos agrupados mediante el uso del enfoque común, conocido como split-apply-combine strategy. Agrupa los datos según cierta condición, aplica una función a cada grupo y luego los combina de nuevo en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "spark2 = SparkSession.builder.appName(\"example2\").getOrCreate()\n",
    "# Crea un DataFrame en PySpark con datos proporcionados y un esquema definido\n",
    "dfg = spark2.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "dfg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Agrupar y luego aplicar la función avg() a los grupos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfg.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">También puedes aplicar una función nativa de Python a cada grupo utilizando la API de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define una función llamada plus_mean que toma un DataFrame de pandas como entrada,\n",
    "# y devuelve el DataFrame con la columna 'v1' ajustada restando la media de 'v1'.\n",
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
    "# Agrupa el DataFrame 'df' por la columna 'color' y aplica la función plus_mean a cada grupo utilizando applyInPandas.\n",
    "# Muestra el resultado con el mismo esquema que el DataFrame original.\n",
    "dfg.groupby('color').applyInPandas(plus_mean, schema=dfg.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Co-agrupando y aplicando una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+----+\n",
      "|    time| id| v1|  v2|\n",
      "+--------+---+---+----+\n",
      "|20000101|  1|1.0|   x|\n",
      "|20000102|  1|3.0|NULL|\n",
      "|20000101|  2|2.0|   y|\n",
      "|20000102|  2|4.0|NULL|\n",
      "+--------+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark2.stop()\n",
    "spark3 = SparkSession.builder.appName(\"example3\").getOrCreate()\n",
    "# Crea dos DataFrames en PySpark con datos proporcionados y esquemas definidos\n",
    "dfc1 = spark3.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    ('time', 'id', 'v1'))\n",
    "dfc2 = spark3.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "# Define una función llamada merge_ordered que realiza una fusión ordenada de dos DataFrames de pandas.\n",
    "def merge_ordered(l, r):\n",
    "    return pd.merge_ordered(l, r)\n",
    "# Realiza una operación de co-agrupamiento en los DataFrames 'df1' y 'df2' por la columna 'id',\n",
    "# luego aplica la función merge_ordered a cada grupo utilizando applyInPandas.\n",
    "# Muestra el resultado con un esquema especificado.\n",
    "dfc1.groupby('id').cogroup(dfc2.groupby('id')).applyInPandas(\n",
    "    merge_ordered, schema='time int, id int, v1 double, v2 string').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando SQL\n",
    "> Los DataFrames y Spark SQL comparten el mismo motor de ejecución, por lo que se pueden utilizar de manera intercambiable sin problemas. Para esto, tienes que crear una vista temporal del DataFrame que es una representación lógica de los datos del DF permitiendo usar SQL con SparkSQL. Por ejemplo, puedes registrar el DataFrame como una tabla y ejecutar una consulta SQL fácilmente de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark3.stop()\n",
    "spark4 = SparkSession.builder.appName(\"example4\").getOrCreate()\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = [(\"Alice\", 28), (\"Bob\", 22), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df_example = spark4.createDataFrame(data, columns)\n",
    "# Registrar el DataFrame como una vista temporal llamada \"people\"\n",
    "df_example.createOrReplaceTempView(\"people\")\n",
    "# Ejecutar una consulta SQL tradicional utilizando Spark SQL\n",
    "result = spark4.sql(\"SELECT Name, Age FROM people WHERE Age >= 30\")\n",
    "# Mostrar el resultado en la consola\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark4.stop()\n",
    "spark5 = SparkSession.builder.appName(\"example5\").getOrCreate()\n",
    "# Crea un DataFrame en PySpark con datos proporcionados y un esquema definido\n",
    "df = spark5.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Creando una vista temporal de la tabla, se puede acceder a esta como si fuera una tabla de SQL noraml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registra el DataFrame 'df' como una vista temporal llamada \"tableA\"\n",
    "df.createOrReplaceTempView(\"tableA\")\n",
    "# Ejecuta una consulta SQL utilizando Spark SQL, contando el número de filas en la vista \"tableA\"\n",
    "spark5.sql(\"SELECT count(*) FROM tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">También, UDFs pueden requistrarse e invocarse en SQL directo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|add_one(v1)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "|          5|\n",
      "|          6|\n",
      "|          7|\n",
      "|          8|\n",
      "|          9|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "spark5.udf.register(\"add_one\", add_one)\n",
    "spark5.sql(\"SELECT add_one(v1) FROM tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Estas consultas de SQL pueden combinarse y usarsae con columnas de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|add_one(v1)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "|          5|\n",
      "|          6|\n",
      "|          7|\n",
      "|          8|\n",
      "|          9|\n",
      "+-----------+\n",
      "\n",
      "+--------------+\n",
      "|(count(1) > 0)|\n",
      "+--------------+\n",
      "|          true|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Selecciona la columna resultante de la función 'add_one' aplicada a la columna 'v1'\n",
    "df.selectExpr('add_one(v1)').show()\n",
    "# Selecciona un valor booleano que indica si la cuenta de todas las filas es mayor que 0\n",
    "df.select(expr('count(*) > 0')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregaciones\n",
    ">El método agg se utiliza para realizar agregaciones en un DataFrame. Se utiliza comúnmente para calcular estadísticas agregadas o realizar operaciones de resumen en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Crear el DataFrame para ejemplificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 28|\n",
      "|    Bob| 22|\n",
      "|Charlie| 35|\n",
      "|  Alice| 40|\n",
      "|    Bob| 25|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark5.stop()\n",
    "spark6 = SparkSession.builder.appName(\"example5\").getOrCreate()\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = [(\"Alice\", 28), (\"Bob\", 22), (\"Charlie\", 35), (\"Alice\", 40), (\"Bob\", 25)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark6.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Puede usarse con notación de diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(Age)|\n",
      "+--------+\n",
      "|      40|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Age': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Se realiza la agregación calculando el promedio de la columna \"Age\" y el recuento de nombres. La función avg se utiliza para calcular el promedio, y count se utiliza para contar el número de registros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|   Name|average_age|name_count|\n",
      "+-------+-----------+----------+\n",
      "|    Bob|       23.5|         2|\n",
      "|  Alice|       34.0|         2|\n",
      "|Charlie|       35.0|         1|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar agregaciones usando agg\n",
    "result = df.groupBy(\"Name\").agg(avg(\"Age\").alias(\"average_age\"), count(\"Name\").alias(\"name_count\"))\n",
    "# Mostrar el resultado\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuentes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
